{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is used to exclude rDNA in KARR-seq and map snoRNA-non-rRNA interactions\n",
    "# in this pipeline, fastp is used to merge and dedupe paired end data\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pysam\n",
    "import re\n",
    "import pybedtools\n",
    "samtools = '/tools/bin/samtools'\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "STAR = '/tools/bin/STAR'\n",
    "fastp = '/tools/bin/fastp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aaf0e-e0c4-4c2d-bec3-7366ccf5ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIGARdiv(CIGAR):\n",
    "    #considers all CIGAR operations [MINDSPH=X]\n",
    "    #divide a cigar string to N-separated segments, used in functions below\n",
    "    #return these results:\n",
    "    #gaps, a list of N strings, e.g. ['100N', '200N']\n",
    "    #segs, a list of segments, each with all possible operations except N\n",
    "    #gaplens, a list of N lengths, e.g. [100, 200]\n",
    "    #Glen, genomic length of the entire alignment (consumed ref) [MDN=X]\n",
    "    #Mlens, segment lengths of matches only [M=X]\n",
    "    #Rlens, segment lengths of consumed Reference [MD=X]\n",
    "    #Qlens, segment lengths of consumed Query [MIS=X]\n",
    "    #example: \n",
    "    #Glen,gaps,gaplens,segs,Mlens,Qlens,Rlens = CIGARdiv(CIGAR)\n",
    "    #or replace unwanted output with '_' \n",
    "    gaps=re.findall('\\d+N', CIGAR)\n",
    "    gaplens=[int(gap[:-1]) for gap in gaps] #gap lengths \n",
    "    segs=[i.rstrip('0123456789') for i in CIGAR.split('N')]\n",
    "    Mlens=[sum([int(i[:-1]) for i in re.findall('\\d+[M=X]',s)]) for s in segs]\n",
    "    Rlens=[sum([int(i[:-1]) for i in re.findall('\\d+[MD=X]',s)]) for s in segs]\n",
    "    Qlens=[sum([int(i[:-1]) for i in re.findall('\\d+[MIS=X]',s)]) for s in segs]\n",
    "    Glen=sum(gaplens+Rlens)\n",
    "    return Glen, gaps, gaplens, segs, Mlens, Qlens, Rlens\n",
    "\n",
    "def get_bam_len(path):\n",
    "    '''\n",
    "    count the length of a bam file given the path\n",
    "    '''\n",
    "    cmd = f\"{samtools} view -@ 40 -c {path}\"\n",
    "    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, shell=True)\n",
    "    bam_len = int(p.communicate()[0])\n",
    "    return bam_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7075163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory and sample names\n",
    "input_prefix = '/home/beiliu/data/220125_snoKARR_seq/'\n",
    "\n",
    "samplename = ['HepG2_50ASO_ribo+_1','HepG2_50ASO__ribo+_2',\n",
    "              'HepG2_50ASO__ribo-_1', 'HepG2_50ASO__ribo-_2',\n",
    "              'HepG2_100ASO__ribo-_1', 'HepG2_100ASO_ribo-_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1ce44",
   "metadata": {},
   "source": [
    "# Pre-process fasta file using fastp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612ee1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a list of input gz file names\n",
    "input_names = os.listdir(f'{input_prefix}')\n",
    "input_names = sorted([n for n in input_names if n.endswith('gz')])\n",
    "if len(input_names)!=2*len(samplename):\n",
    "    print('!!!!!!warning, double check input_names and samplename')\n",
    "input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce97f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trimed_dir = f'{input_prefix}/fastp_processed'\n",
    "os.system(f'mkdir {trimed_dir}')\n",
    "for i in trange(int(len(input_names)/2)):\n",
    "    # merge and trim adapter\n",
    "    input1 = f'{input_prefix}/' + input_names[i*2]\n",
    "    input2 = f'{input_prefix}/' + input_names[i*2+1]\n",
    "    print(f'inputs are\\n 1.{input1}\\n 2.{input2}\\n')\n",
    "    \n",
    "    os.system(f'{fastp} --in1 {input1} \\\n",
    "    --in2 {input2} \\\n",
    "    -m \\\n",
    "    --out1 {trimed_dir}/unmerged_out1.fasta.gz \\\n",
    "    --out2 {trimed_dir}/unmerged_out2.fasta.gz \\\n",
    "    --merged_out {trimed_dir}/merged{i+1}.fasta.gz \\\n",
    "    -w 16 \\\n",
    "    -h {trimed_dir}/merge_and_trim_adapter_{i+1}.html \\\n",
    "    -j {trimed_dir}/merge_and_trim_adapter_{i+1}.json')\n",
    "    \n",
    "    # dedupe and trim XXX\n",
    "    os.system(f'{fastp} -i {trimed_dir}/merged{i+1}.fasta.gz \\\n",
    "    -D -t 3 -w 16 \\\n",
    "    -o {trimed_dir}/merged{i+1}_dedupe.fasta.gz \\\n",
    "    -h {trimed_dir}/dedupe_and_trim_xxx_{i+1}.html \\\n",
    "    -j {trimed_dir}/dedupe_and_trim_xxx_{i+1}.json')\n",
    "    \n",
    "os.system(f'rm {trimed_dir}/unmerged*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e380c",
   "metadata": {},
   "source": [
    "# Map to pure rDNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7e2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_dir_rDNA = '/home/beiliu/Database/Ref/human_rDNA_repeat/pure_rDNA/STAR_index'\n",
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "#     if i ==0:\n",
    "#         continue\n",
    "    name = '{:03d}_{}_round1'.format(i+1, name_prefix)\n",
    "    input1 = f'{input_prefix}/fastp_processed/merged{i+1}_dedupe.fasta.gz'\n",
    "    print(f'1st round STAR alignment input is {input1}')\n",
    "    os.system(f'{STAR} --runThreadN 40 \\\n",
    "    --runMode alignReads \\\n",
    "    --genomeDir {gen_dir_rDNA} \\\n",
    "    --readFilesIn {input1}  \\\n",
    "    --outFileNamePrefix {input_prefix}/star_dedupe_alignment_pure_rDNA/{name} \\\n",
    "    --readFilesCommand zcat \\\n",
    "    --outReadsUnmapped Fastx  \\\n",
    "    --outFilterMultimapNmax 10 \\\n",
    "    --outFilterScoreMinOverLread 0 \\\n",
    "    --outFilterMatchNminOverLread 0 \\\n",
    "    --outSAMattributes All \\\n",
    "    --outSAMtype BAM Unsorted \\\n",
    "    --alignIntronMin 1 \\\n",
    "    --scoreGap 0 \\\n",
    "    --scoreGapNoncan 0 \\\n",
    "    --scoreGapGCAG 0 \\\n",
    "    --scoreGapATAC 0 \\\n",
    "    --scoreGenomicLengthLog2scale -1 \\\n",
    "    --chimFilter None \\\n",
    "    --chimOutType WithinBAM HardClip \\\n",
    "    --chimSegmentMin 5 \\\n",
    "    --chimJunctionOverhangMin 5 \\\n",
    "    --chimScoreJunctionNonGTAG 0 \\\n",
    "    --chimScoreDropMax 80 \\\n",
    "    --chimNonchimScoreDropMin 20 \\\n",
    "    --peOverlapNbasesMin 12 \\\n",
    "    --peOverlapMMp 0.05 \\\n",
    "    --limitOutSJcollapsed 10000000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b845c86",
   "metadata": {},
   "source": [
    "# First round map to genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701f4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map unmapped fasta file to genome\n",
    "# first round\n",
    "gen_dir = '/home/beiliu/Database/Ref/STAR_index'\n",
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    if i ==0:\n",
    "        continue\n",
    "    name = '{:03d}_{}_ex_rDNA_round1'.format(i+1, name_prefix)\n",
    "    name_0 = '{:03d}_{}_round1'.format(i+1, name_prefix)\n",
    "    input1 = f'{input_prefix}/star_dedupe_alignment_pure_rDNA/{name_0}Unmapped.out.mate1'\n",
    "    print(f'1st round STAR alignment input is {input1}')\n",
    "    os.system(f'{STAR} --runThreadN 40 \\\n",
    "    --runMode alignReads \\\n",
    "    --genomeDir {gen_dir} \\\n",
    "    --readFilesIn {input1}  \\\n",
    "    --outFileNamePrefix {input_prefix}/star_dedupe_alignment_exrDNA_1st/{name} \\\n",
    "    --readFilesCommand cat \\\n",
    "    --outReadsUnmapped Fastx  \\\n",
    "    --outFilterMultimapNmax 20 \\\n",
    "    --chimMultimapNmax 100 \\\n",
    "    --outFilterScoreMinOverLread 0 \\\n",
    "    --outFilterMatchNminOverLread 0 \\\n",
    "    --outSAMattributes All \\\n",
    "    --outSAMtype BAM SortedByCoordinate \\\n",
    "    --alignIntronMin 1 \\\n",
    "    --scoreGap 0 \\\n",
    "    --scoreGapNoncan 0 \\\n",
    "    --scoreGapGCAG 0 \\\n",
    "    --scoreGapATAC 0 \\\n",
    "    --scoreGenomicLengthLog2scale -1 \\\n",
    "    --chimFilter None \\\n",
    "    --chimOutType WithinBAM HardClip \\\n",
    "    --chimSegmentMin 5 \\\n",
    "    --chimJunctionOverhangMin 5 \\\n",
    "    --chimScoreJunctionNonGTAG 0 \\\n",
    "    --chimScoreDropMax 80 \\\n",
    "    --chimNonchimScoreDropMin 20 \\\n",
    "    --peOverlapNbasesMin 12 \\\n",
    "    --peOverlapMMp 0.05 \\\n",
    "    --limitOutSJcollapsed 10000000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8773ff9",
   "metadata": {},
   "source": [
    "# Softreverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e36b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minlen = 5 #minimal length of the softclipped shorter segment\n",
    "os.system('mkdir {}/star_dedupe_alignment_exrDNA_1st'.format(input_prefix))\n",
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    # specify input and output bam files\n",
    "    name = '{:03d}_{}_ex_rDNA_round1'.format(i+1, name_prefix)\n",
    "    bamfile_path = f'{input_prefix}/star_dedupe_alignment_exrDNA_1st/{name}' +\\\n",
    "    'Aligned.sortedByCoord.out.bam'\n",
    "    inputbam = pysam.AlignmentFile(f'{bamfile_path}', 'rb')\n",
    "    outputbam = pysam.AlignmentFile('{}/star_dedupe_alignment_exrDNA_1st/{:03d}_{}_converted.bam'.format(input_prefix, i+1, name_prefix), \n",
    "                                    'wb', template = inputbam)\n",
    "\n",
    "    #get bam file length\n",
    "    cmd = f\"{samtools} view -@ 40 {bamfile_path} | wc -l\"\n",
    "    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, shell=True)\n",
    "    bam_len = int(p.communicate()[0])\n",
    "\n",
    "    # Loop through the bam file and perform the reverse\n",
    "    count = 0\n",
    "    for j, line in tqdm(enumerate(inputbam), total=bam_len):\n",
    "        count+=1\n",
    "        FLAG = line.flag\n",
    "        if FLAG>=256 and bin(FLAG)[-9]=='1' or line.has_tag('SA'):\n",
    "            continue #ignore secondary (FLAG=256) and chiastic alignments (SA:Z).\n",
    "        CIGAR = line.cigarstring\n",
    "        subMS = re.findall('\\d+[MS]', CIGAR) #substrings for M and S\n",
    "        softs = re.findall('\\d+S', CIGAR)\n",
    "        softslen = [int(k[:-1]) for k in softs]    \n",
    "        if softslen and max(softslen) >= minlen:\n",
    "            QNAME, SEQ, QUAL = line.qname, line.seq, line.qual\n",
    "            if 'S' in subMS[0] and 'S' not in subMS[-1]:\n",
    "                SEQn = SEQ[softslen[0]:] + SEQ[:softslen[0]]\n",
    "                QUALn = QUAL[softslen[0]:] + QUAL[:softslen[0]]\n",
    "            elif 'S' not in subMS[0] and 'S' in subMS[-1]:\n",
    "                SEQn = SEQ[-softslen[0]:] + SEQ[:-softslen[0]]\n",
    "                QUALn = SEQ[-softslen[0]:] + SEQ[:-softslen[0]]\n",
    "            elif 'S' in subMS[0] and 'S' in subMS[-1]:\n",
    "                SEQn = SEQ[-softslen[1]:] + \\\n",
    "                      SEQ[softslen[0]:-softslen[1]] + SEQ[:softslen[0]] \n",
    "                QUALn = QUAL[-softslen[1]:] + \\\n",
    "                       QUAL[softslen[0]:-softslen[1]] + QUAL[:softslen[0]]  \n",
    "            line.seq = SEQn\n",
    "            line.qual = QUALn\n",
    "            outputbam.write(line)\n",
    "\n",
    "    inputbam.close()\n",
    "    outputbam.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9b782",
   "metadata": {},
   "source": [
    "# convert softreverse bam to fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ad326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    # convert converted_bam to converted fastq\n",
    "    os.system(f'{samtools} bam2fq -@ 40 \\\n",
    "    {input_prefix}/star_dedupe_alignment_exrDNA_1st/{i+1:03d}_{name_prefix}_converted.bam \\\n",
    "    | gzip > {input_prefix}/star_dedupe_alignment_exrDNA_1st/{i+1:03d}_{name_prefix}_converted.fastq.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe434d",
   "metadata": {},
   "source": [
    "# 2nd round of mapping to genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12601d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    name = '{:03d}_{}_exrDNA_round2'.format(i+1, name_prefix)\n",
    "    input1 = f'{input_prefix}/star_dedupe_alignment_exrDNA_1st/{i+1:03d}_{name_prefix}_converted.fastq.gz'\n",
    "    print(f'2nd round STAR alignment input is {input1}')\n",
    "    os.system(f'{STAR} --runThreadN 40 \\\n",
    "    --runMode alignReads \\\n",
    "    --genomeDir {gen_dir} \\\n",
    "    --readFilesIn {input1}  \\\n",
    "    --outFileNamePrefix {input_prefix}/star_dedupe_alignment_exrDNA_2nd/{name} \\\n",
    "    --readFilesCommand zcat \\\n",
    "    --outReadsUnmapped Fastx  \\\n",
    "    --outFilterMultimapNmax 20 \\\n",
    "    --chimMultimapNmax 100 \\\n",
    "    --outFilterScoreMinOverLread 0 \\\n",
    "    --outFilterMatchNminOverLread 0 \\\n",
    "    --outSAMattributes All \\\n",
    "    --outSAMtype BAM SortedByCoordinate \\\n",
    "    --alignIntronMin 1 \\\n",
    "    --scoreGap 0 \\\n",
    "    --scoreGapNoncan 0 \\\n",
    "    --scoreGapGCAG 0 \\\n",
    "    --scoreGapATAC 0 \\\n",
    "    --scoreGenomicLengthLog2scale -1 \\\n",
    "    --chimFilter None \\\n",
    "    --chimOutType WithinBAM HardClip \\\n",
    "    --chimSegmentMin 5 \\\n",
    "    --chimJunctionOverhangMin 5 \\\n",
    "    --chimScoreJunctionNonGTAG 0 \\\n",
    "    --chimScoreDropMax 80 \\\n",
    "    --chimNonchimScoreDropMin 20 \\\n",
    "    --peOverlapNbasesMin 12 \\\n",
    "    --peOverlapMMp 0.05 \\\n",
    "    --limitOutSJcollapsed 10000000 \\\n",
    "    --limitBAMsortRAM 50657646430')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe90dd",
   "metadata": {},
   "source": [
    "# combine bam files from 2 rounds of mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecafa802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directory for combined bam files from 2 rounds of mapping\n",
    "os.system(f'mkdir -p {input_prefix}/bam_from_2_rounds_exrDNA')\n",
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    bam_1 = '{}/star_dedupe_alignment_exrDNA_1st/{:03d}_{}_ex_rDNA_round1'.format(input_prefix, i+1, name_prefix) +\\\n",
    "    'Aligned.sortedByCoord.out.bam'\n",
    "    bam_2 = '{}/star_dedupe_alignment_exrDNA_2nd/{:03d}_{}_exrDNA_round2'.format(input_prefix, i+1, name_prefix) +\\\n",
    "    'Aligned.sortedByCoord.out.bam'\n",
    "    os.system(f'{samtools} merge -@ 40 -f \\\n",
    "    {input_prefix}/bam_from_2_rounds_exrDNA/{i+1:03d}_{name_prefix}_exrDNA_total.bam {bam_1} {bam_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686792b7-0428-4886-9193-38bb401e22f0",
   "metadata": {},
   "source": [
    "# get chimeric bam and process chimeric reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd325eb-7e53-48bb-96cb-5c012fe16672",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg38_rRNA_bed = pybedtools.BedTool('/home/beiliu/Database/Repeats/UCSC_repeat_masker_rRNA.bed')\n",
    "snoDB_bed = pybedtools.BedTool('/home/beiliu/Database/Bed_files/snoDB.bed')\n",
    "repeat_bed = pybedtools.BedTool('/home/beiliu/Database/Repeats/UCSC_repeat_masker.bed')\n",
    "genome_gtf_path = '/home/beiliu/Database/annotation/hg38/gencode.v39.annotation.gtf'\n",
    "ref = 'hg38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8047f-f10b-4328-a406-d482c15d0e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    # if i == 0:\n",
    "    #     continue\n",
    "    output_foldername = f'{input_prefix}/bam_from_2_rounds_exrDNA/{i+1:03d}_chimeric_reads_processing'\n",
    "    os.system(f'mkdir -p {output_foldername}')\n",
    "    \n",
    "    bamfile = f'{input_prefix}/bam_from_2_rounds_exrDNA/{i+1:03d}_{name_prefix}_exrDNA_total.bam'\n",
    "    \n",
    "    # first, extract chimeric reads from bam file\n",
    "    cmd1 = f'{samtools} view -@ 40 -Shb -e \\\"[SA]\\\" \\\n",
    "    {bamfile}  > {output_foldername}/chimall.bam'\n",
    "    os.system(cmd1)\n",
    "    \n",
    "    # 2nd, extract primary alignment from bam file\n",
    "    os.system(f'{samtools} view -hub -@ 40 -F 1284 \\\n",
    "    {output_foldername}/chimall.bam > {output_foldername}/chimall_primary.bam')\n",
    "    \n",
    "    # 3rd directly overlap chimall.bam with snoDB.bed:  \n",
    "    os.system(f'bedtools intersect -abam {output_foldername}/chimall_primary.bam -b \\\n",
    "    ~/Database/Bed_files/snoDB.bed -S -u > {output_foldername}/snoDB_overlapped_chimall.bam')\n",
    "    \n",
    "    # 4. get unique qname from snoDB_overlapped_chimall.bam\n",
    "    os.system(f'{samtools} view -@ 40 {output_foldername}/snoDB_overlapped_chimall.bam \\\n",
    "    | cut -f1 | sort | uniq > {output_foldername}/snoDB_qname.txt')\n",
    "    \n",
    "    # 5. select reads from chimall.bam that contains qname from snoDB_overlapped_chimall.bam\n",
    "    os.system(f'{samtools} view -@ 60 -N {output_foldername}/snoDB_qname.txt \\\n",
    "    -o {output_foldername}/snoDB_chimall_filtered_qname.bam {output_foldername}/chimall_primary.bam')\n",
    "    \n",
    "    # 6. use pysam to read all alignment and get chr, start, cigar, strand information of both arms and generate bed\n",
    "    chimall_path = f'{output_foldername}/snoDB_chimall_filtered_qname.bam'\n",
    "    input_chimall_bam = pysam.AlignmentFile(chimall_path, 'rb')\n",
    "    chimall_bam_len = get_bam_len(chimall_path)\n",
    "    \n",
    "    # 7. generate chimeric reads bed files\n",
    "    with open(f'{output_foldername}/snoDB_chimall_filtered_qname_sorted_left.bed', 'w') as f1, \\\n",
    "    open (f'{output_foldername}/snoDB_chimall_filtered_qname_sorted_right.bed', 'w') as f2:\n",
    "        for i, line in tqdm(enumerate(input_chimall_bam), total=chimall_bam_len):\n",
    "            ID = i+1\n",
    "            qname = line.qname\n",
    "            chr1, pos1, strand1, cigar1 = \\\n",
    "                    line.reference_name, line.pos+1, \"-\" if line.is_reverse else '+', line.cigarstring\n",
    "            chr2, pos2, strand2, cigar2, _, _ = [i for i in line.tags if i[0]=='SA'][0][1].split(',')\n",
    "            pos2 = int(pos2)\n",
    "\n",
    "            end1 = pos1+CIGARdiv(cigar1)[0]-1\n",
    "            end2 = int(pos2)+CIGARdiv(cigar2)[0]-1\n",
    "\n",
    "            # get rid of intra-molecular interactions\n",
    "            if chr1 == chr2:\n",
    "                overlap = max(0, min(end1, end2) - max(pos1, pos2))\n",
    "                if overlap != 0:\n",
    "                    continue\n",
    "\n",
    "            # correct the start position for bed file output\n",
    "            line1 = [chr1, str(pos1-1), str(end1), str(ID), qname, strand1]\n",
    "            line1 = '\\t'.join(line1)\n",
    "            # print(line)\n",
    "            f1.write(f\"{line1}\\n\")\n",
    "\n",
    "            # correct the start position for bed file output\n",
    "            line2 = [chr2, str(pos2-1), str(end2), str(ID), qname, strand2]\n",
    "            line2 = '\\t'.join(line2)\n",
    "            # print(line)\n",
    "            f2.write(f\"{line2}\\n\")\n",
    "            \n",
    "    # 8. process chimeric bed files\n",
    "    # next overlap left and right arm with rRNA to exclude rRNA\n",
    "    left_bed = pybedtools.BedTool(f'{output_foldername}/snoDB_chimall_filtered_qname_sorted_left.bed')\n",
    "    right_bed = pybedtools.BedTool(f'{output_foldername}/snoDB_chimall_filtered_qname_sorted_right.bed')\n",
    "\n",
    "    # remove rRNA\n",
    "    left_norRNA = left_bed.intersect(hg38_rRNA_bed, v = True, wa = True, S = True).to_dataframe()\n",
    "    right_norRNA = right_bed.intersect(hg38_rRNA_bed, v = True, wa = True, S = True).to_dataframe()       \n",
    "    \n",
    "    \n",
    "    # merge left and right that have the same ID (neither arm overlaps with rRNA)\n",
    "    norRNA_chimeric_df = pd.merge(left_norRNA, right_norRNA, on = 'name', how = 'inner')\n",
    "\n",
    "    # create full nema columns to sort each row\n",
    "    norRNA_chimeric_df['arm1'] = norRNA_chimeric_df['chrom_x'] + '$' +\\\n",
    "    norRNA_chimeric_df['start_x'].astype(str) + '$' +\\\n",
    "    norRNA_chimeric_df['end_x'].astype(str) + '$' +\\\n",
    "    norRNA_chimeric_df['strand_x'] + '$' + \\\n",
    "    norRNA_chimeric_df['name'].astype(str) + '$' +\\\n",
    "    norRNA_chimeric_df['score_x']\n",
    "\n",
    "    norRNA_chimeric_df['arm2'] = norRNA_chimeric_df['chrom_y'] + '$' +\\\n",
    "    norRNA_chimeric_df['start_y'].astype(str) + '$' +\\\n",
    "    norRNA_chimeric_df['end_y'].astype(str) + '$' +\\\n",
    "    norRNA_chimeric_df['strand_y'] + '$' + \\\n",
    "    norRNA_chimeric_df['name'].astype(str) + '$' +\\\n",
    "    norRNA_chimeric_df['score_y']\n",
    "    \n",
    "    # sort each row\n",
    "    df = norRNA_chimeric_df[['arm1', 'arm2']]\n",
    "    sort_row_chim = pd.DataFrame(np.sort(df.values, axis=1)[:,::-1], \n",
    "                 index=df.index, \n",
    "                 columns=df.columns)\n",
    "    # recover coordinates\n",
    "    arm1_df = sort_row_chim['arm1'].str.split('$', expand = True)\n",
    "    arm2_df = sort_row_chim['arm2'].str.split('$', expand = True)\n",
    "\n",
    "    # concat two arms\n",
    "    arm1_arm2_df = pd.concat([arm1_df, arm2_df], axis=1)\n",
    "\n",
    "    arm1_arm2_df.columns = ['chr1', 'start1', 'end1', 'strand1', 'ID', 'qname', \n",
    "                           'chr2', 'start2', 'end2', 'strand2', 'ID2', 'qname2']\n",
    "\n",
    "    arm1_arm2_df = arm1_arm2_df[['chr1', 'start1', 'end1', 'strand1', 'ID', 'qname', \n",
    "                           'chr2', 'start2', 'end2', 'strand2']]\n",
    "\n",
    "    # drop qname duplicates, each chimeric reads have two entries with the same qname\n",
    "    arm1_arm2_df = arm1_arm2_df.drop_duplicates(subset = 'qname').reset_index(drop=True)\n",
    "    \n",
    "    # now make arm1 snoRNA and arm2 target\n",
    "    arm1_bed_df = arm1_arm2_df[['chr1','start1','end1','ID','qname','strand1']]\n",
    "    arm2_bed_df = arm1_arm2_df[['chr2','start2','end2','ID','qname','strand2']]\n",
    "\n",
    "    arm1_bed = pybedtools.BedTool.from_dataframe(arm1_bed_df)\n",
    "    arm2_bed = pybedtools.BedTool.from_dataframe(arm2_bed_df)\n",
    "\n",
    "    # intersect left and right arms with snoDB\n",
    "    arm1_bed_snoRNA = arm1_bed.intersect(snoDB_bed, wa = True, wb = True, u = True, S = True)\n",
    "    arm2_bed_snoRNA = arm2_bed.intersect(snoDB_bed, wa = True, wb = True, u = True, S = True)\n",
    "\n",
    "    arm1_bed_snoRNA_df = arm1_bed_snoRNA.to_dataframe()\n",
    "    arm2_bed_snoRNA_df = arm2_bed_snoRNA.to_dataframe()\n",
    "\n",
    "    # left arm is snoRNA\n",
    "    left_arm_snoRNA = arm1_arm2_df[arm1_arm2_df['qname'].isin(arm1_bed_snoRNA_df['score'])]\n",
    "    # right arm is snoRNA, switch chr1 and chr2 coordinates\n",
    "    right_arm_snoRNA = arm1_arm2_df[arm1_arm2_df['qname'].isin(arm2_bed_snoRNA_df['score'])]\n",
    "    right_arm_snoRNA = right_arm_snoRNA[['chr2','start2','end2','strand2', 'ID','qname', 'chr1','start1','end1','strand1']]\n",
    "    right_arm_snoRNA.columns = left_arm_snoRNA.columns\n",
    "\n",
    "    # drop duplicates that both arms are snoRNAs\n",
    "    reordered_snoRNA_target_df = pd.concat([left_arm_snoRNA, right_arm_snoRNA]).drop_duplicates(subset = 'qname')\n",
    "    \n",
    "    # now split reordered_snoRNA_target_df to left and right bed files, run annotation\n",
    "    snoRNA_bed_df = reordered_snoRNA_target_df[['chr1','start1','end1','qname','ID','strand1']]\n",
    "    target_bed_df = reordered_snoRNA_target_df[['chr2','start2','end2','qname','ID','strand2']]\n",
    "\n",
    "    pybedtools.BedTool.from_dataframe(snoRNA_bed_df).saveas(f'{output_foldername}/snoRNA.bed')\n",
    "    pybedtools.BedTool.from_dataframe(target_bed_df).saveas(f'{output_foldername}/target.bed')\n",
    "\n",
    "    # remove intermediate files\n",
    "    os.system(f'rm {output_foldername}/*.bam')\n",
    "    os.system(f'rm {output_foldername}/snoDB_qname.txt')\n",
    "    os.system(f'rm {output_foldername}/snoDB_chimall_filtered_qname_sorted_*.bed')\n",
    "\n",
    "    # Run annotation for both arms\n",
    "    with open (f'{output_foldername}/annotate1.sh', 'w') as hd:\n",
    "        hd.write(f'#!/bin/bash\\nexport PATH=/home/beiliu/anaconda3/bin/:$PATH\\nannotatePeaks.pl \\\n",
    "        {output_foldername}/snoRNA.bed {ref} -gtf {genome_gtf_path}\\\n",
    "        -annStats {output_foldername}/snoRNA_stats_annote.txt -cpu 40 > {output_foldername}/snoRNA_annotate.txt')\n",
    "    os.system(f'nohup bash {output_foldername}/annotate1.sh &')\n",
    "\n",
    "    with open (f'{output_foldername}/annotate2.sh', 'w') as hd:\n",
    "        hd.write(f'#!/bin/bash\\nexport PATH=/home/beiliu/anaconda3/bin/:$PATH\\nannotatePeaks.pl \\\n",
    "        {output_foldername}/target.bed {ref} -gtf {genome_gtf_path}\\\n",
    "        -annStats {output_foldername}/target_stats_annote.txt -cpu 40 > {output_foldername}/target_annotate.txt')\n",
    "    os.system(f'nohup bash {output_foldername}/annotate2.sh &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567bd88-b0f2-4b2c-abc9-52cf4e557716",
   "metadata": {},
   "source": [
    "# process annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e22b86-374c-45b7-8a15-d08a30172fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    data_dir = f'{input_prefix}/bam_from_2_rounds_exrDNA/{i+1:03d}_chimeric_reads_processing'\n",
    "    \n",
    "    # read in annotation files\n",
    "    anote_snoRNA = pd.read_csv(f'{data_dir}/snoRNA_annotate.txt', delimiter='\\t')\n",
    "    anote_snoRNA = anote_snoRNA.rename(columns={anote_snoRNA.columns[0]:'ID'})\n",
    "    anote_snoRNA = anote_snoRNA[['Chr', 'Start', 'End', 'Strand', 'Annotation', 'Detailed Annotation', 'Distance to TSS', 'Gene Name', 'Gene Type', 'ID']]\n",
    "\n",
    "    anote_target = pd.read_csv(f'{data_dir}/target_annotate.txt', delimiter='\\t')\n",
    "    anote_target = anote_target.rename(columns={anote_target.columns[0]:'ID'})\n",
    "    anote_target = anote_target[['Chr', 'Start', 'End', 'Strand', 'Annotation', 'Detailed Annotation', 'Distance to TSS', 'Gene Name', 'Gene Type', 'ID']]\n",
    "    \n",
    "    # directly annotate snoRNA using snoDB\n",
    "    snoRNA_total_bed = pybedtools.BedTool.from_dataframe(anote_snoRNA[['Chr', 'Start', 'End', 'ID', 'Gene Name', 'Strand']])\n",
    "    totalsnoRNA_snoDB_annot = snoRNA_total_bed.intersect(snoDB_bed, wa = True, wb = True, S = True).to_dataframe()\n",
    "    snoDB_snoRNA_annotation = totalsnoRNA_snoDB_annot.drop_duplicates('name')[['name', 'blockCount']]\n",
    "    snoDB_snoRNA_annotation.columns = ['ID', 'snoRNA_snoDB_annotation']\n",
    "\n",
    "    # directly annotate snoRNA using repeat bed\n",
    "    totalsnoRNA_snoDB_annot_repeat = snoRNA_total_bed.intersect(repeat_bed, wa = True, wb = True, S = True).to_dataframe()\n",
    "    totalsnoRNA_snoDB_annot_repeat = totalsnoRNA_snoDB_annot_repeat.drop_duplicates('name')[['name', 'blockCount']]\n",
    "    totalsnoRNA_snoDB_annot_repeat.columns = ['ID', 'snoRNA_repeat_annotation']\n",
    "\n",
    "    # merge annotation and snoDB annotation\n",
    "    snoRNA_target_annotation_df = reduce(lambda left, right: pd.merge(left, right, on = 'ID', how = 'inner'), \n",
    "                                         [anote_snoRNA, anote_target, snoDB_snoRNA_annotation])\n",
    "    snoRNA_target_annotation_df = snoRNA_target_annotation_df.dropna()\n",
    "    snoRNA_target_annotation_df = snoRNA_target_annotation_df[~snoRNA_target_annotation_df['Gene Type_y'].str.contains('rRNA|snoRNA')]\n",
    "    snoRNA_target_annotation_df.to_csv(f'{data_dir}/snoRNA_target_annotation.csv', index = False)\n",
    "    \n",
    "    # merge annotation total and snoRNA annotated from repeat\n",
    "    repeat_annotated_snoRNA_df = pd.merge(snoRNA_target_annotation_df, totalsnoRNA_snoDB_annot_repeat, on = 'ID', how = 'inner')\n",
    "    repeat_annotated_snoRNA_df.to_csv(f'{data_dir}/snoRNA_target_annotation_snoRNA_annotated_by_repeat.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fab92e-815e-40ee-9032-653c1202ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all annotation files\n",
    "total_annotation_lst = []\n",
    "snoRNA_annotatedbyrepeats_annotation_lst = []\n",
    "for i, name_prefix in tqdm(enumerate(samplename), total = len(samplename)):\n",
    "    # if i == 0:\n",
    "    #     continue\n",
    "    data_dir = f'{input_prefix}/bam_from_2_rounds_exrDNA/{i+1:03d}_chimeric_reads_processing'\n",
    "    \n",
    "    total_annotation = pd.read_csv(f'{data_dir}/snoRNA_target_annotation.csv')\n",
    "    snoRNA_annotatedbyrepeats_annotation = pd.read_csv(f'{data_dir}/snoRNA_target_annotation_snoRNA_annotated_by_repeat.csv')\n",
    "    \n",
    "    total_annotation_lst.append(total_annotation)\n",
    "    snoRNA_annotatedbyrepeats_annotation_lst.append(snoRNA_annotatedbyrepeats_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b298a57-a7f4-4a26-8590-284e7be6dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique chromosome of snoRNA and targets for the first two data\n",
    "data1 = total_annotation_lst[0]\n",
    "data2 = total_annotation_lst[1]\n",
    "\n",
    "# the idea is that for each chr and strand in left arm and right arm, run DG clustering\n",
    "snoRNA_common_chr = np.intersect1d(data1['Chr_x'].unique(), data2['Chr_x'].unique())\n",
    "target_common_chr = np.intersect1d(data1['Chr_y'].unique(), data2['Chr_y'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c01cfa-7391-479a-b5eb-45d7b2eab953",
   "metadata": {},
   "source": [
    "# Run DG clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f7ae4-3208-48ff-b2e3-a73b8b655eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/KARR_seq/Process_function_notebooks/DG_sim.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7398d3-64b7-4163-bbc9-0b392b424646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DG_dir = f'{input_prefix}/bam_from_2_rounds_exrDNA/DG_clustering_001_002'\n",
    "os.system(f'mkdir -p {DG_dir}')\n",
    "\n",
    "# for the first two replicate data\n",
    "# loop through the snoRNA and target chromosome, run DG clustering\n",
    "for i, chr1 in tqdm(enumerate(snoRNA_common_chr), total = len(snoRNA_common_chr)):\n",
    "\n",
    "    print(f'processing snoRNA chromosome {chr1}')\n",
    "    # split + and - dataset\n",
    "    chr1_pos_data1 = data1[(data1['Chr_x']==chr1) & (data1['Strand_x']=='+')]\n",
    "    chr1_pos_data2 = data2[(data2['Chr_x']==chr1) & (data2['Strand_x']=='+')]\n",
    "    chr1_neg_data1 = data1[(data1['Chr_x']==chr1) & (data1['Strand_x']=='-')]\n",
    "    chr1_neg_data2 = data2[(data2['Chr_x']==chr1) & (data2['Strand_x']=='-')]\n",
    "    for j, chr2 in tqdm(enumerate(target_common_chr), total = len(target_common_chr)):\n",
    "\n",
    "        print(f'processing target chromosome {chr2}')\n",
    "        chr1_pos_chr2_pos_data1 = chr1_pos_data1[(chr1_pos_data1['Chr_y']==chr2) & (chr1_pos_data1['Strand_y']=='+')]\n",
    "        chr1_pos_chr2_pos_data2 = chr1_pos_data2[(chr1_pos_data2['Chr_y']==chr2) & (chr1_pos_data2['Strand_y']=='+')]\n",
    "        \n",
    "        chr1_pos_chr2_neg_data1 = chr1_pos_data1[(chr1_pos_data1['Chr_y']==chr2) & (chr1_pos_data1['Strand_y']=='-')]\n",
    "        chr1_pos_chr2_neg_data2 = chr1_pos_data2[(chr1_pos_data2['Chr_y']==chr2) & (chr1_pos_data2['Strand_y']=='-')]\n",
    "        \n",
    "        chr1_neg_chr2_pos_data1 = chr1_neg_data1[(chr1_neg_data1['Chr_y']==chr2) & (chr1_neg_data1['Strand_y']=='+')]\n",
    "        chr1_neg_chr2_pos_data2 = chr1_neg_data2[(chr1_neg_data2['Chr_y']==chr2) & (chr1_neg_data2['Strand_y']=='+')]\n",
    "        \n",
    "        chr1_neg_chr2_neg_data1 = chr1_neg_data1[(chr1_neg_data1['Chr_y']==chr2) & (chr1_neg_data1['Strand_y']=='-')]\n",
    "        chr1_neg_chr2_neg_data2 = chr1_neg_data2[(chr1_neg_data2['Chr_y']==chr2) & (chr1_neg_data2['Strand_y']=='-')]\n",
    "        \n",
    "        # run clustering for each variation\n",
    "        for m,n in zip([chr1_pos_chr2_pos_data1, chr1_pos_chr2_neg_data1, chr1_neg_chr2_pos_data1, chr1_neg_chr2_neg_data1], \n",
    "                      [('+', '+'), ('+', '-'), ('-', '+'), ('-', '-')]):\n",
    "            data_filtred = m[['Chr_x','Start_x','End_x','Chr_y','Start_y','End_y']].reset_index(drop=True)\n",
    "            data_filtred.columns = ['gene1', 'start1', 'end1', 'gene2', 'start2', 'end2']\n",
    "            run_total_clustering_nonsnoRNA(data_filtred, 0.4, f'{chr1}_{n[0]}', f'{chr2}_{n[1]}', csv_name = f'{DG_dir}/{chr1}_{n[0]}_{chr2}_{n[1]}_rep1', threshold = 1)\n",
    "            \n",
    "        for m,n in zip([chr1_pos_chr2_pos_data2, chr1_pos_chr2_neg_data2, chr1_neg_chr2_pos_data2, chr1_neg_chr2_neg_data2], \n",
    "                      [('+', '+'), ('+', '-'), ('-', '+'), ('-', '-')]):\n",
    "            data_filtred = m[['Chr_x','Start_x','End_x','Chr_y','Start_y','End_y']].reset_index(drop=True)\n",
    "            data_filtred.columns = ['gene1', 'start1', 'end1', 'gene2', 'start2', 'end2']\n",
    "            run_total_clustering_nonsnoRNA(data_filtred, 0.4, f'{chr1}_{n[0]}', f'{chr2}_{n[1]}', csv_name = f'{DG_dir}/{chr1}_{n[0]}_{chr2}_{n[1]}_rep2', threshold = 1)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
